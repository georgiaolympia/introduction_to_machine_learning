{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning \n",
    "\n",
    "## Exercise 4 - Multi-Class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook covers a Python-based solution for the third programming exercise of the machine learning class on Coursera.  Please refer to the [exercise text](https://github.com/jdwittenauer/ipython-notebooks/blob/master/exercises/ML/ex3.pdf) for detailed descriptions and equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For this exercise we'll use logistic regression to recognize hand-written digits (0 to 9).  We'll be extending the implementation of linear regression we wrote in Exercise 2 and apply it to one-vs-all classification.  \n",
    "\n",
    "Read the sections *Introduction* and 1.1 as well as 1.2 of the detailed exercise description.\n",
    "\n",
    "Then get started by loading the data set.  The data set is in a MATLAB's native format, so to load it in Python we need to use a SciPy library.\n",
    "\n",
    "If you want to refresh your knoweledge of Neural Networks in the context of this exercise check out sections 2 of the exercise descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "%matplotlib inline\n",
    "\n",
    "# Load and display data using loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determin the dimensionality of your data, to varify that all samples got loaded correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we've got our data loaded.  The images are represented in martix X as a 400-dimensional vector (of which there are 5,000 of them).  The 400 \"features\" are grayscale intensities of each pixel in the original 20 x 20 image.  The class labels are in the vector y as a numeric class representing the digit that's in the image.\n",
    "\n",
    "The first task is to modify our linear regression implementation into a logistic regression in completely vectorized form (i.e. no \"for\" loops).  We need vectorized code, because in addition to being short and concise, is able to take advantage of linear algebra optimizations and is typically much faster than iterative code.\n",
    "\n",
    "If you take a look at our cost function implementation from the Exercise 2 Solution, you can see that it's already vectorized!  So we can re-use that same implementation here and only need to take care of modifying it into a classfication.\n",
    "We will use the Sigmoid function as our activation function, so start by implementing that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-3-8ae6f53a3378>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-8ae6f53a3378>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    # TODO\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now continue with the cost function. You can find the precise mathematical formula to be implemented here in section 1.3.1 of the detailed exercise description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(theta, X, y, learningRate):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need the function that computes the gradient.  Again, we already defined this in the previous exercise, only in this case we want to get rid of the \"for\" loop in the update step.  Here's the original code for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_with_loop(theta, X, y, learningRate):\n",
    "    theta = np.matrix(theta)\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    \n",
    "    parameters = int(theta.ravel().shape[1])\n",
    "    grad = np.zeros(parameters)\n",
    "    \n",
    "    error = sigmoid(X * theta.T) - y\n",
    "    \n",
    "    for i in range(parameters):\n",
    "        term = np.multiply(error, X[:,i])\n",
    "        \n",
    "        if (i == 0):\n",
    "            grad[i] = np.sum(term) / len(X)\n",
    "        else:\n",
    "            grad[i] = (np.sum(term) / len(X)) + ((learningRate / len(X)) * theta[:,i])\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our new version we're going to pull out the \"for\" loop and compute the gradient for all the values of theta at once using linear algebra.  To follow the math behind the transformation, refer to the exercise 3 text section 1.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient(theta, X, y, learningRate):\n",
    "   # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined our cost and gradient functions, it's time to build a classifier.  For this task we've got 10 possible classes (one for every digit), and since logistic regression is only able to distiguish between 2 classes at a time, we need a strategy to deal with the multi-class scenario.\n",
    "\n",
    "Read section 1.4 of the detailed exercise description to understand the approach we will take.\n",
    "\n",
    "The idea: We immplement a one-vs-all classification approach, where a label with k different classes results in k classifiers, each one deciding between \"class i\" and \"not class i\" (i.e. any class other than i).  We're going to wrap the classifier training up in one function that computes the final weights for each of the 10 classifiers and returns the weights as a k X (n + 1) array, where n is the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def one_vs_all(X, y, num_labels, learning_rate):\n",
    "    rows = X.shape[0]\n",
    "    params = X.shape[1]\n",
    "    \n",
    "    # k X (n + 1) array for the parameters of each of the k classifiers\n",
    "    # TODO\n",
    "    \n",
    "    # insert a column of ones at the beginning for the intercept term (same as in Exercise 2)\n",
    "    # TODO\n",
    "    \n",
    "    # iterate over all labels (labels are 1-indexed instead of 0-indexed)\n",
    "    for i in range(1, num_labels + 1):\n",
    "        #TODO \n",
    "        \n",
    "        # minimize the objective function\n",
    "        # don't do this by hand! Use scipys optimization API\n",
    "        #TODO \n",
    "    \n",
    "    return all_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to note here...first, we'are adding an extra parameter to theta (along with a column of ones to the training data) to account for the intercept term.  Second, we're transforming y from a class label to a binary value for each classifier (either is class i or is not class i).  Finally, we're using SciPy's newer optimization API to minimize the cost function for each classifier.  The API takes an objective function, an initial set of parameters, an optimization method, and a jacobian (gradient) function if specified.  The parameters found by the optimization routine are then assigned to the parameter array.\n",
    "\n",
    "One of the more challenging parts of implementing vectorized code is getting all of the matrix interactions written correctly, so I find it useful to do some sanity checks by looking at the shapes of the arrays/matrices I'm working with and convincing myself that they're sensible :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000L, 401L), (5000L, 1L), (401L,), (10L, 401L))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do some checking on our data\n",
    "\n",
    "rows = data['X'].shape[0]\n",
    "params = data['X'].shape[1]\n",
    "\n",
    "all_theta = np.zeros((10, params + 1))\n",
    "\n",
    "X = np.insert(data['X'], 0, values=np.ones(rows), axis=1)\n",
    "\n",
    "theta = np.zeros(params + 1)\n",
    "\n",
    "y_0 = np.array([1 if label == 0 else 0 for label in data['y']])\n",
    "y_0 = np.reshape(y_0, (rows, 1))\n",
    "\n",
    "X.shape, y_0.shape, theta.shape, all_theta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These all appear to make sense.  Note that theta is a one-dimensional array, so when it gets converted to a matrix in the code that computes the gradient, it turns into a (1 X 401) matrix.  Let's also check the class labels in y to make sure they look like what we're expecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(data['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that our training function actually runs, and we get some sensible outputs, before going any further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -5.79312170e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          1.22140973e-02,   2.88611969e-07,   0.00000000e+00],\n",
       "       [ -4.91685285e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          2.40449128e-01,  -1.08488270e-02,   0.00000000e+00],\n",
       "       [ -8.56840371e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "         -2.59241796e-04,  -1.12756844e-06,   0.00000000e+00],\n",
       "       ..., \n",
       "       [ -1.32641613e+01,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "         -5.63659404e+00,   6.50939114e-01,   0.00000000e+00],\n",
       "       [ -8.55392716e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "         -2.01206880e-01,   9.61930149e-03,   0.00000000e+00],\n",
       "       [ -1.29807876e+01,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          2.60651472e-04,   4.22693052e-05,   0.00000000e+00]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_theta = one_vs_all(data['X'], data['y'], 10, 1)\n",
    "all_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready for the final step - using the trained classifiers to predict a label for each image.  For this step we're going to compute the class probability for each class, for each training instance (using vectorized code of course!) and assign the output class label as the class with the highest probability.\n",
    "Check out section 1.4.1 of the exercise description for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_all(X, all_theta):\n",
    "    #TODO \n",
    "    return h_argmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the predict_all function to generate class predictions for each instance and see how well our classifier works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 97.58%\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_all(data['X'], all_theta)\n",
    "correct = [1 if a == b else 0 for (a, b) in zip(y_pred, data['y'])]\n",
    "accuracy = (sum(map(int, correct)) / float(len(correct)))\n",
    "print 'accuracy = {0}%'.format(accuracy * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost 98% isn't too bad!  That's all for exercise 3.  In the next exercise, we'll look at how to implement a feed-forward neural network from scratch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
